{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_costs(x, y, z, theta):\n",
    "    costs = (np.matmul(x,theta)-z)*(1-2*y)\n",
    "    return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_classifier(x, costs):\n",
    "    f = []\n",
    "    for cost in costs.T:\n",
    "        f.append(LinearRegression(fit_intercept=False).fit(x, cost))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_grad(objective, x, z, theta, n):\n",
    "    zhat = np.matmul(x,theta)\n",
    "    zdiff = zhat - z\n",
    "    \n",
    "    if objective=='MSE':\n",
    "        grad = 2*np.matmul(x.T,zdiff)/n\n",
    "        #Do we really want \"flatten\" here? This goes from a multi- to 1-D array\n",
    "        return grad.flatten()\n",
    "    \n",
    "    elif objective=='dot_product':\n",
    "        grad = np.matmul(x.T,z)/n\n",
    "        return grad.flatten()\n",
    "    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_converged(x, size, epsilon, d):\n",
    "    for i in range(0,size):\n",
    "        for j in range(0,d):\n",
    "            if np.linalg.norm(x[-1:][j] - x[-(2+i):][j], np.inf) > epsilon:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_proxies(x, y, z_all, C, iters, n, K, d, epsilon, theta_true, objective=None):\n",
    "    theta_list = []\n",
    "    grad_list = []\n",
    "    theta_average_list = []\n",
    "    \n",
    "    for k in range(0,K):\n",
    "        #Be very careful about dimensionality here -- again, reduce to simplest case\n",
    "        z = z_all[:,k].reshape(-1,1)\n",
    "\n",
    "        theta = [np.random.rand(d,1)]\n",
    "        grad_l = [0]\n",
    "        theta_average = [0]\n",
    "        converged = False\n",
    "        \n",
    "        for t in range(1, iters):\n",
    "            if converged:\n",
    "                break\n",
    "                \n",
    "            costs = linear_costs(x, y, z, theta[t-1])\n",
    "            f_s = paired_classifier(x, costs)\n",
    "            \n",
    "            ###\n",
    "            #Something is weird here with dimensions of h\n",
    "            #Separate this for multiple y case and write version for one y -- good sanity check\n",
    "            \n",
    "            h = [0]*n\n",
    "            val = []\n",
    "            \n",
    "            for i, f in enumerate(f_s):\n",
    "                h_s = f.predict(x)\n",
    "                h_plus = (h_s > 0)\n",
    "                h_minus = (h_s < 0)\n",
    "                if np.abs(np.sum(h_s[h_plus]))>np.abs(np.sum(h_s[h_minus])):\n",
    "                    h[i] = h_plus\n",
    "                else:\n",
    "                    h[i] = h_minus\n",
    "                val.append(np.sum(h_s[h[i]]))\n",
    "                \n",
    "            y_index = np.argmin(val)\n",
    "            y_temp = y[:,y_index]\n",
    "            h = h[y_index].astype(int)\n",
    "            ####\n",
    "            \n",
    "            zhat = np.matmul(x,theta[t-1])\n",
    "            zhat_sum = np.sum(zhat)\n",
    "            z_sum = np.sum(z)\n",
    "            \n",
    "            #TODO: Add this at end, might correct things\n",
    "            err_points = np.abs(h-y_temp)\n",
    "            err_cost = np.matmul((zhat-z).T,err_points)\n",
    "            overall_diff = (zhat_sum/z_sum) - 1\n",
    "            \n",
    "            #Double check that types work out here, kinda weird with flatten\n",
    "            if np.abs(overall_diff) >= np.abs(err_cost):\n",
    "                penalty = np.sign(overall_diff) * np.sum(x, axis=0)/z_sum\n",
    "            else:\n",
    "                penalty = np.sign(err_cost) * np.matmul(np.transpose(x),err_points)  \n",
    "                penalty = penalty.flatten()\n",
    "            \n",
    "            #Work on MSE objective\n",
    "            grad_l.append(obj_grad(objective,x,z,theta[t-1],n) + C * penalty) \n",
    "            \n",
    "            #Can we cut down storage space to speed up here?\n",
    "            theta.append(theta[t-1] - (np.power(t, -1/2) * grad_l[t]).reshape(-1,1))\n",
    "            theta_average.append((t*theta_average[t-1]+theta[t])/(t+1))\n",
    "            \n",
    "            if t > 10:\n",
    "                if has_converged(theta_average, 10, epsilon, 1):\n",
    "                    print(\"Converged\")\n",
    "                    converged = True\n",
    "            \n",
    "        theta_list.append(theta)\n",
    "        grad_list.append(grad_l)\n",
    "        theta_average_list.append(theta_average[-1])\n",
    "        #TODO: sanity check that theta_average_list entry is same thing as average of thetas\n",
    "        \n",
    "    return theta_list, grad_list, theta_average_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hyperparameters(d, B, z, n, M, alpha):\n",
    "    z_sum = np.sum(z)\n",
    "    alpha_exp = alpha*z_sum/(1+n*M)\n",
    "    \n",
    "    C = (M**2+2*alpha_exp)/alpha_exp\n",
    "    T = d**4*np.square(2*M*B+n*C*B/z_sum)/np.square(alpha_exp)\n",
    "    return C, int(T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make into function\n",
    "#4\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "#n=10\n",
    "n = 5\n",
    "m = 1\n",
    "K = n\n",
    "d = n\n",
    "\n",
    "M = 2\n",
    "B = 1\n",
    "\n",
    "epsilon = 0.00001\n",
    "alpha=0.1\n",
    "trials=5\n",
    "\n",
    "intercept = np.ones(n).reshape(-1,1)\n",
    "discrepancy = []\n",
    "    \n",
    "for i in range(0,trials):\n",
    "    z_train = np.round(np.random.rand(n,K))\n",
    "    \n",
    "    #Append intercept or lack thereof here? Can we make sub structures? I think number of groups \n",
    "    #might be getting confused with something else\n",
    "    \n",
    "    theta = np.random.rand(n,n)\n",
    "    x_train = np.matmul(z_train, np.linalg.inv(theta))\n",
    "    x_train = np.hstack((intercept, x_train))\n",
    "    y_train = np.round(np.random.rand(n,m))\n",
    "\n",
    "    #Revisit this\n",
    "    C, T = calculate_hyperparameters((d+1), B, z_train, n, M, alpha)\n",
    "    #Fix up these parameters with new structure\n",
    "    \n",
    "    #z_train = z_train[:,0].reshape(-1,1)\n",
    "    \n",
    "    coefficients, gradients, theta_average = linear_proxies(x_train, y_train, z_train, C, T, n, 1, (d+1), epsilon, theta[:,0])\n",
    "    \n",
    "    z_train = z_train[:,0].reshape(-1,1)\n",
    "    \n",
    "    final_costs = linear_costs(x_train, y_train, z_train, theta_average)\n",
    "    \n",
    "    [prc] = paired_classifier(x_train, final_costs)\n",
    "    \n",
    "    h_s = prc.predict(x_train)\n",
    "    h_plus = (h_s > 0)\n",
    "    h_minus = (h_s < 0)\n",
    "    \n",
    "    if np.abs(np.sum(h_s[h_plus]))>np.abs(np.sum(h_s[h_minus])):\n",
    "        h = h_plus\n",
    "    else:\n",
    "        h = h_minus\n",
    "\n",
    "    h_int = h.astype(int)\n",
    "    h_err = (h_int != y_train)\n",
    "    z_hat = np.matmul(x_train, theta_average)[0]\n",
    "    print(z_hat)\n",
    "    \n",
    "    for i in range(0,1):\n",
    "        if sum(h_err[:,i])==0:\n",
    "            discrepancy.append(0)\n",
    "        else:\n",
    "            discrepancy.append(np.mean(z_train[h_err[:,i]])/(np.mean(z_train)) - np.mean(z_hat[h_err[:,i]])/(np.mean(z_hat)))\n",
    "    \n",
    "print(discrepancy)\n",
    "print(np.mean(discrepancy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(discrepancy)\n",
    "\n",
    "#Portion of discrepancies that fall in good range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
